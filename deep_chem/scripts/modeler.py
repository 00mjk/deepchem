"""
Top level script to featurize input, train models, and evaluate them.
"""
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals
import argparse
import os
from deep_chem.utils.featurize import generate_directories
from deep_chem.utils.featurize import extract_data
from deep_chem.utils.featurize import generate_targets
from deep_chem.utils.featurize import generate_features
from deep_chem.utils.featurize import generate_vs_utils_features
from deep_chem.utils.featurize import featurize_inputs
from deep_chem.models.standard import fit_singletask_models
from deep_chem.utils.load import process_datasets
from deep_chem.utils.load import transform_data
from deep_chem.utils.save import save_model
from deep_chem.utils.save import load_model
from deep_chem.utils.save import save_sharded_dataset
from deep_chem.utils.save import load_sharded_dataset
from deep_chem.utils.evaluate import results_to_csv
from deep_chem.utils.evaluate import eval_trained_model
from deep_chem.utils.evaluate import compute_model_performance
from deep_chem.utils.preprocess import train_test_split
from deep_chem.utils.fit import fit_model


def add_featurization_command(subparsers):
  """Adds flags for featurize subcommand."""
  featurize_cmd = subparsers.add_parser(
      "featurize", help="Featurize raw input data.")
  add_featurize_group(featurize_cmd)

def add_featurize_group(featurize_cmd):
  """Adds flags for featurizization."""
  featurize_group = featurize_cmd.add_argument_group("Input Specifications")
  featurize_group.add_argument(
      "--input-files", required=1, nargs="+",
      help="Input file with data.")
  featurize_group.add_argument(
      "--input-type", default="csv",
      choices=["csv", "pandas", "sdf"],
      help="Type of input file. If pandas, input must be a joblib\n"
           "containing a pandas dataframe. If sdf, should be in\n"
           "(perhaps gzipped) sdf file.")
  featurize_group.add_argument(
      "--delimiter", default=",", type=str,
      help="If csv input, delimiter to use for read csv file")
  featurize_group.add_argument(
      "--fields", required=1, nargs="+",
      help="Names of fields.")
  featurize_group.add_argument(
      "--field-types", required=1, nargs="+",
      choices=["string", "float", "list-string", "list-float", "ndarray"],
      help="Type of data in fields.")
  featurize_group.add_argument(
      "--feature-fields", type=str, nargs="+",
      help="Optional field that holds pre-computed feature vector")
  featurize_group.add_argument(
      "--target-fields", type=str, nargs="+", required=1,
      help="Name of measured field to predict.")
  featurize_group.add_argument(
      "--split-field", type=str, default=None,
      help="Name of field specifying train/test split.")
  featurize_group.add_argument(
      "--smiles-field", type=str, default="smiles",
      help="Name of field specifying SMILES for molecule.")
  featurize_group.add_argument(
      "--id-field", type=str, default=None,
      help="Name of field specifying unique identifier for molecule.\n"
           "If none is specified, then smiles-field is used as identifier.")
  # TODO(rbharath): This should be moved to train-tests-split
  featurize_group.add_argument(
      "--threshold", type=float, default=None,
      help="If specified, will be used to binarize real-valued target-fields.")
  featurize_group.add_argument(
      "--name", required=1,
      help="Name of the dataset.")
  featurize_group.add_argument(
      "--out", required=1,
      help="Folder to generate processed dataset in.")
  featurize_group.set_defaults(func=featurize_inputs_wrapper)

def add_train_test_command(subparsers):
  """Adds flags for train-test-split subcommand."""
  train_test_cmd = subparsers.add_parser(
      "train-test-split",
      help="Apply standard data transforms to raw features generated by featurize,\n"
           "then split data into train/test and store data as (X,y) matrices.")
  train_test_cmd.add_argument(
      "--input-transforms", nargs="+", default=[],
      choices=["normalize-and-truncate"],
      help="Transforms to apply to input data.")
  train_test_cmd.add_argument(
      "--output-transforms", type=str, default="",
      help="Comma-separated list (no spaces) of transforms to apply to output data.\n"
           "Supported transforms are 'log' and 'normalize'. 'None' will be taken\n"
           "to mean no transforms are required.")
  train_test_cmd.add_argument(
      "--feature-types", type=str, required=1,
      help="Comma-separated list (no spaces) of types of featurizations to use.\n"
           "Each featurization must correspond to subdirectory in generated\n"
           "data directory.")
  train_test_cmd.add_argument(
      "--paths", nargs="+", required=1,
      help="Paths to input datasets.")
  train_test_cmd.add_argument(
      "--splittype", type=str, default="scaffold",
      choices=["scaffold", "random", "specified"],
      help="Type of train/test data-splitting. 'scaffold' uses Bemis-Murcko scaffolds.\n"
           "specified requires that split be in original data.")
  train_test_cmd.add_argument(
      "--weight-positives", type=bool, default=False,
      help="Weight positive examples to have same total weight as negatives.")
  train_test_cmd.add_argument(
      "--mode", default="singletask",
      choices=["singletask", "multitask"],
      help="Type of model being built.")
  train_test_cmd.add_argument(
      "--train-out", type=str, required=1,
      help="Location to save train set.")
  train_test_cmd.add_argument(
      "--test-out", type=str, required=1,
      help="Location to save test set.")
  train_test_cmd.set_defaults(func=train_test_split_wrapper)

def add_model_group(fit_cmd):
  """Adds flags for specifying models."""
  group = fit_cmd.add_argument_group("model")
  group.add_argument(
      "--model", required=1,
      choices=["logistic", "rf_classifier", "rf_regressor",
               "linear", "ridge", "lasso", "lasso_lars", "elastic_net",
               "singletask_deep_classifier", "multitask_deep_classifier",
               "singletask_deep_regressor", "multitask_deep_regressor",
               "convolutional_3D_regressor"],
      help="Type of model to build. Some models may allow for\n"
           "further specification of hyperparameters. See flags below.")

  group = fit_cmd.add_argument_group("Neural Net Parameters")
  group.add_argument(
      "--nb-hidden", type=int, default=500,
      help="Number of hidden neurons for NN models.")
  group.add_argument(
      "--learning-rate", type=float, default=0.01,
      help="Learning rate for NN models.")
  group.add_argument(
      "--dropout", type=float, default=0.5,
      help="Learning rate for NN models.")
  group.add_argument(
      "--nb-epoch", type=int, default=50,
      help="Number of epochs for NN models.")
  group.add_argument(
      "--batch-size", type=int, default=32,
      help="Number of examples per minibatch for NN models.")
  group.add_argument(
      "--loss-function", type=str, default="mean_squared_error",
      help="Loss function type.")
  group.add_argument(
      "--decay", type=float, default=1e-4,
      help="Learning rate decay for NN models.")
  group.add_argument(
      "--validation-split", type=float, default=0.0,
      help="Percent of training data to use for validation.")


def add_fit_command(subparsers):
  """Adds arguments for fit subcommand."""
  fit_cmd = subparsers.add_parser(
      "fit", help="Fit a model to training data.")
  group = fit_cmd.add_argument_group("load-and-transform")
  group.add_argument(
      "--saved-data", required=1,
      help="Location of saved transformed data.")
  add_model_group(fit_cmd)
  group = fit_cmd.add_argument_group("save")
  group.add_argument(
      "--saved-out", type=str, required=1,
      help="Location to save trained model.")
  fit_cmd.set_defaults(func=fit_model_wrapper)


def add_eval_command(subparsers):
  """Adds arguments for eval subcommand."""
  eval_cmd = subparsers.add_parser(
      "eval",
      help="Evaluate trained model on test data processed by transform.")
  group = eval_cmd.add_argument_group("load model/data")
  group.add_argument(
      "--saved-model", type=str, required=1,
      help="Location from which to load saved model.")
  group.add_argument(
      "--saved-data", required=1, help="Location of saved transformed data.")
  group.add_argument(
      "--model_type", required=1,
      choices=["sklearn", "keras-graph", "keras-sequential"],
      help="Type of model to load.")
  eval_cmd.add_argument(
      "--csv-out", type=str, required=1,
      help="Outputted predictions on evaluated set.")
  eval_cmd.add_argument(
      "--stats-out", type=str, required=1j,
      help="Computed statistics on evaluated set.")
  eval_cmd.set_defaults(func=eval_trained_model_wrapper)

# TODO(rbharath): There are a lot of duplicate commands introduced here. Is
# there a nice way to factor them?
def add_model_command(subparsers):
  """Adds flags for model subcommand."""
  model_cmd = subparsers.add_parser(
      "model", help="Combines featurize, train-test-split, fit, eval into one\n"
      "command for user convenience.")
  model_cmd.add_argument(
      "--skip-featurization", action="store_true",
      help="If set, skip the featurization step.")
  model_cmd.add_argument(
      "--skip-train-test-split", action="store_true",
      help="If set, skip the train-test-split step.")
  model_cmd.add_argument(
      "--skip-fit", action="store_true",
      help="If set, skip model fit step.")
  add_featurize_group(model_cmd)

  train_test_group = model_cmd.add_argument_group("train_test_group")
  train_test_group.add_argument(
      "--input-transforms", nargs="+", default=[],
      choices=["normalize-and-truncate"],
      help="Transforms to apply to input data.")
  train_test_group.add_argument(
      "--output-transforms", type=str, default="",
      help="Comma-separated list (no spaces) of transforms to apply to output data.\n"
           "Supported transforms are log and normalize.")
  train_test_group.add_argument(
      "--mode", default="singletask",
      choices=["singletask", "multitask"],
      help="Type of model being built.")
  train_test_group.add_argument(
      "--feature-types", type=str, required=1,
      help="Comma-separated list (no spaces) of types of featurizations to use.\n"
           "Each featurization must correspond to subdirectory in generated\n"
           "data directory.")
  train_test_group.add_argument(
      "--splittype", type=str, default="scaffold",
      choices=["scaffold", "random", "specified"],
      help="Type of train/test data-splitting. 'scaffold' uses Bemis-Murcko scaffolds.\n"
           "specified requires that split be in original data.")

  add_model_group(model_cmd)
  model_cmd.set_defaults(func=create_model)

def extract_training_params(args):
  params = ["nb_hidden", "learning_rate", "dropout",
            "nb_epoch", "decay", "batch_size", "loss_function"]

  training_params = {param : getattr(args, param) for param in params}
  return(training_params)

def create_model(args):
  """Creates a model"""
  data_dir = os.path.join(args.out, args.name)
  print("+++++++++++++++++++++++++++++++++")
  print("Perform featurization")
  if not args.skip_featurization:
    featurize_inputs(
        args.name, args.out, args.input_file, args.input_type, args.fields,
        args.field_types, args.feature_fields, args.target_fields,
        args.smiles_field, args.split_field, args.id_field, args.threshold,
        args.delimiter)

  print("+++++++++++++++++++++++++++++++++")
  print("Perform train-test split")
  paths = [data_dir]
  train_out = os.path.join(data_dir, "%s-train.joblib" % args.name)
  test_out = os.path.join(data_dir, "%s-test.joblib" % args.name)
  if not args.skip_train_test_split:
    train_test_split(
        paths, args.output_transforms, args.input_transforms, args.feature_types,
        args.splittype, args.mode, train_out, test_out,
        args.target_fields)

  print("+++++++++++++++++++++++++++++++++")
  print("Fit model")
  model_type = get_model_type(args.model)
  extension = get_model_extension(model_type)
  saved_out = os.path.join(data_dir, "%s.%s" % (args.model, extension))
  if not args.skip_fit:
    training_params = extract_training_params(args)
    fit_model(
        args.model, training_params,
        args.validation_split, saved_out, train_out, args.target_fields)


  print("+++++++++++++++++++++++++++++++++")
  print("Eval Model on Train")
  print("-------------------")
  csv_out_train = os.path.join(data_dir, "%s-train.csv" % args.name)
  stats_out_train = os.path.join(data_dir, "%s-train-stats.txt" % args.name)
  csv_out_test = os.path.join(data_dir, "%s-test.csv" % args.name)
  stats_out_test = os.path.join(data_dir, "%s-test-stats.txt" % args.name)
  eval_trained_model(
      model_type, saved_out, train_out, csv_out_train, 
      stats_out_train, args.target_fields)
  print("Eval Model on Test")
  print("------------------")
  eval_trained_model(
      model_type, saved_out, test_out, csv_out_test, 
      stats_out_test, args.target_fields)

def parse_args(input_args=None):
  """Parse command-line arguments."""
  parser = argparse.ArgumentParser()
  subparsers = parser.add_subparsers(title='Modes')

  add_featurization_command(subparsers)
  add_train_test_command(subparsers)
  add_fit_command(subparsers)
  add_eval_command(subparsers)

  add_model_command(subparsers)

  return parser.parse_args(input_args)

def featurize_inputs_wrapper(args):
  """Wrapper function that calls _featurize_input with args unwrapped."""
  featurize_inputs(
      args.name, args.out, args.input_file, args.input_type, args.fields,
      args.field_types, args.feature_fields, args.target_fields,
      args.smiles_field, args.split_field, args.id_field, args.threshold,
      args.delimiter)

def train_test_split_wrapper(args):
  """Wrapper function that calls _train_test_split_wrapper after unwrapping args."""
  preprocess.train_test_split(
      args.paths, args.output_transforms, args.input_transforms,
      args.feature_types, args.splittype, args.mode,
      args.train_out, args.test_out, args.target_fields)

def fit_model_wrapper(args):
  """Wrapper that calls _fit_model with arguments unwrapped."""
  training_params = extract_training_params(args)
  fit_model(
      args.model, training_params, args.validation_split,
      args.saved_out, args.saved_data, args.target_fields)



def get_model_type(model):
  """Associate each model with a model_type (used for saving/loading)."""
  if model in ["singletask_deep_network", "multitask_deep_network"]:
    model_type = "keras-graph"
  elif model in ["3D_cnn"]:
    model_type = "keras-sequential"
  elif model == "neural_fingerprint":
    model_type = "autograd"
  else:
    model_type = "sklearn"
  return model_type

def get_model_extension(model_type):
  """Get the saved filetype extension for various types of models."""
  if model_type == "sklearn":
    return "joblib"
  elif model_type == "autograd":
    return "joblib.gz"
  elif model_type == "keras-graph" or model_type == "keras-sequential":
    return "h5"

def eval_trained_model_wrapper(args):
  """Wrapper function that calls _eval_trained_model with unwrapped args."""
  eval_trained_model(
      args.model_type, args.saved_model, args.saved_data,
      args.csv_out, args.stats_out, args.target_fields)



def main():
  """Invokes argument parser."""
  args = parse_args()
  args.func(args)

if __name__ == "__main__":
  main()
