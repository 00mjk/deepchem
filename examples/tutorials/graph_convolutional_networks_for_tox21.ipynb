{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Part 3: Introduction to Graph Convolutions\n",
    "\n",
    "In the previous sections of the tutorial, we learned about `Dataset` and `Model` objects. We learned how to load some data into DeepChem from files on disk and also learned some basic facts about molecular data handling. We then dove into some basic deep learning architectures and explored DeepChem's `TensorGraph` framework for deep learning. However, until now, we stuck with vanilla deep learning architectures and didn't really consider how to handle deep architectures specifically engineered to work with life science data.\n",
    "\n",
    "In this tutorial, we'll change that by going a little deeper and learn about \"graph convolutions.\" These are one of the most powerful deep learning tools for working with molecular data. The reason for this is that molecules can be naturally viewed as graphs.\n",
    "\n",
    "![Molecular Graph](basic_graphs.gif)\n",
    "\n",
    "Note how standard chemical diagrams of the sort we're used to from high school lend themselves naturally to visualizing molecules as graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use MoleculeNet to load the Tox21 dataset. We need to make sure to process the data in a way that graph convolutional networks can use For that, we make sure to set the featurizer option to 'GraphConv'. The MoleculeNet call will return a training set, an validation set, and a test set for us to use. The call also returns `transformers`, a list of data transformations that were applied to preprocess the dataset. (Most deep networks are quite finicky and require a set of data transformations to ensure that training proceeds stably.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/tox21.csv.gz\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "TIMING: featurizing shard 0 took 8.720 s\n",
      "TIMING: dataset construction took 11.515 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 4.005 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 3.478 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.777 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.655 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load Tox21 dataset\n",
    "tox21_tasks, tox21_datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = tox21_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a graph convolutional network on this dataset. DeepChem has the class `GraphConvModel` that wraps a standard graph convolutional architecture underneath the hood for user convenience. Let's instantiate an object of this class and train it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharath/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 378: Average loss 547.359\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.748 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 547.358907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 756: Average loss 471.955\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.059 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 471.954625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 999: Average loss 442.603\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 1134: Average loss 423.133\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 7.901 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 423.132711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 1512: Average loss 403.883\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.180 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 403.882741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 1890: Average loss 375.246\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.123 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 375.246119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 1999: Average loss 355.777\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 2268: Average loss 347.021\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.921 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 347.020870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 2646: Average loss 322.946\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.391 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 322.946376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 2999: Average loss 301.457\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 3024: Average loss 261.555\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.931 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 261.555212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 3402: Average loss 275.91\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.369 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 275.909556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepchem.models.tensorgraph.tensor_graph:Ending global_step 3780: Average loss 251.993\n",
      "INFO:deepchem.models.tensorgraph.tensor_graph:TIMING: model fitting took 8.376 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 251.993216\n"
     ]
    }
   ],
   "source": [
    "model = GraphConvModel(\n",
    "    len(tox21_tasks), batch_size=50, mode='classification')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('deepchem.models.tensorgraph.tensor_graph')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "num_epochs = 10\n",
    "losses = []\n",
    "for i in range(num_epochs):\n",
    " loss = model.fit(train_dataset, nb_epoch=3)\n",
    " print(\"Epoch %d loss: %f\" % (i, loss))\n",
    " losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these losses so we can take a look at how the loss changes over the process of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f03cc50a588>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "x = range(num_epochs)\n",
    "y = losses\n",
    "plot.scatter(x, y)\n",
    "plot.ylabel(\"L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to evaluate the performance of the model we've trained. For this, we need to define a metric, a measure of model performance. `dc.metrics` holds a collection of metrics already. For this dataset, it is standard to use the ROC-AUC score, the area under the receiver operating characteristic curve (which measures the tradeoff between precision and recall). Luckily, the ROC-AUC score is already available in DeepChem. \n",
    "\n",
    "To measure the performance of the model under this metric, we can use the convenience function `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.80045699830862893, 0.83618637604367374, 0.83908539936708681, 0.77873855933094183, 0.67692252993044244, 0.75578036941489168, 0.75895796821704797, 0.70234314980793855, 0.76387081283102387, 0.65924917162534913, 0.78448201448364341, 0.76675448900822296]\n",
      "Training ROC-AUC Score: 0.760236\n",
      "computed_metrics: [0.71533171721169553, 0.74090608465608465, 0.81106357802757933, 0.70627859684799188, 0.63177272727272715, 0.6326016835811501, 0.61491865697473169, 0.71286314850043442, 0.67676006592889104, 0.51656328658755846, 0.75414979999520937, 0.6603359173126615]\n",
      "Validation ROC-AUC Score: 0.681129\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores[\"mean-roc_auc_score\"])\n",
    "valid_scores = model.evaluate(valid_dataset, [metric], transformers)\n",
    "print(\"Validation ROC-AUC Score: %f\" % valid_scores[\"mean-roc_auc_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on under the hood? Could we build `GraphConvModel` ourselves? Of course! The first step is to create a `TensorGraph` object. This object will hold the \"computational graph\" that defines the computation that a graph convolutional network will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph\n",
    "\n",
    "tg = TensorGraph(use_queue=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the inputs to our model. Conceptually, graph convolutions just requires a the structure of the molecule in question and a vector of features for every atom that describes the local chemical environment. However in practice, due to TensorFlow's limitations as a general programming environment, we have to have some auxiliary information as well preprocessed.\n",
    "\n",
    "`atom_features` holds a feature vector of length 75 for each atom. The other feature inputs are required to support minibatching in TensorFlow. `degree_slice` is an indexing convenience that makes it easy to locate atoms from all molecules with a given degree. `membership` determines the membership of atoms in molecules (atom `i` belongs to molecule `membership[i]`). `deg_adjs` is a list that contains adjacency lists grouped by atom degree For more details, check out the [code](https://github.com/deepchem/deepchem/blob/master/deepchem/feat/mol_graphs.py).\n",
    "\n",
    "To define feature inputs in `TensorGraph`, we use the `Feature` layer. Conceptually, a `TensorGraph` is a mathematical graph composed of layer objects. `Features` layers have to be the root nodes of the graph since they consitute inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Feature\n",
    "\n",
    "atom_features = Feature(shape=(None, 75))\n",
    "degree_slice = Feature(shape=(None, 2), dtype=tf.int32)\n",
    "membership = Feature(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "deg_adjs = []\n",
    "for i in range(0, 10 + 1):\n",
    "    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)\n",
    "    deg_adjs.append(deg_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the body of the graph convolutional network. `TensorGraph` has a number of layers that encode various graph operations. Namely, the `GraphConv`, `GraphPool` and `GraphGather` layers. We will also apply standard neural network layers such as `Dense` and `BatchNorm`.\n",
    "\n",
    "The layers we're adding effect a \"feature transformation\" that will create one vector for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, GraphConv, BatchNorm\n",
    "from deepchem.models.tensorgraph.layers import GraphPool, GraphGather\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "gc1 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[atom_features, degree_slice, membership] + deg_adjs)\n",
    "batch_norm1 = BatchNorm(in_layers=[gc1])\n",
    "gp1 = GraphPool(in_layers=[batch_norm1, degree_slice, membership] + deg_adjs)\n",
    "gc2 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[gp1, degree_slice, membership] + deg_adjs)\n",
    "batch_norm2 = BatchNorm(in_layers=[gc2])\n",
    "gp2 = GraphPool(in_layers=[batch_norm2, degree_slice, membership] + deg_adjs)\n",
    "dense = Dense(out_channels=128, activation_fn=tf.nn.relu, in_layers=[gp2])\n",
    "batch_norm3 = BatchNorm(in_layers=[dense])\n",
    "readout = GraphGather(\n",
    "    batch_size=batch_size,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    in_layers=[batch_norm3, degree_slice, membership] + deg_adjs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make predictions from the `TensorGraph` model. Tox21 is a multitask dataset. That is, there are 12 different datasets grouped together, which share many common molecules, but with different outputs for each. As a result, we have to add a separate output layer for each task. We will use a `for` loop over the `tox21_tasks` list to make this happen. We need to add labels for each\n",
    "\n",
    "We also have to define a loss for the model which tells the network the objective to minimize during training.\n",
    "\n",
    "We have to tell `TensorGraph` which layers are outputs with `TensorGraph.add_output(layer)`. Similarly, we tell the network its loss with `TensorGraph.set_loss(loss)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, SoftMax, \\\n",
    "    SoftMaxCrossEntropy, WeightedError, Stack\n",
    "from deepchem.models.tensorgraph.layers import Label, Weights\n",
    "\n",
    "costs = []\n",
    "labels = []\n",
    "for task in range(len(tox21_tasks)):\n",
    "    classification = Dense(\n",
    "        out_channels=2, activation_fn=None, in_layers=[readout])\n",
    "\n",
    "    softmax = SoftMax(in_layers=[classification])\n",
    "    tg.add_output(softmax)\n",
    "\n",
    "    label = Label(shape=(None, 2))\n",
    "    labels.append(label)\n",
    "    cost = SoftMaxCrossEntropy(in_layers=[label, classification])\n",
    "    costs.append(cost)\n",
    "all_cost = Stack(in_layers=costs, axis=1)\n",
    "weights = Weights(shape=(None, len(tox21_tasks)))\n",
    "loss = WeightedError(in_layers=[all_cost, weights])\n",
    "tg.set_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully defined our graph convolutional model in `TensorGraph`, we need to train it. We can call `fit()`, but we need to make sure that each minibatch of data populates all four `Feature` objects that we've created. For this, we need to create a Python generator that given a batch of data generates a dictionary whose keys are the `Feature` layers and whose values are Numpy arrays we'd like to use for this step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "def data_generator(dataset, epochs=1, predict=False, pad_batches=True):\n",
    "  for epoch in range(epochs):\n",
    "    if not predict:\n",
    "        print('Starting epoch %i' % epoch)\n",
    "    for ind, (X_b, y_b, w_b, ids_b) in enumerate(\n",
    "        dataset.iterbatches(\n",
    "            batch_size, pad_batches=pad_batches, deterministic=True)):\n",
    "      d = {}\n",
    "      for index, label in enumerate(labels):\n",
    "        d[label] = to_one_hot(y_b[:, index])\n",
    "      d[weights] = w_b\n",
    "      multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "      d[atom_features] = multiConvMol.get_atom_features()\n",
    "      d[degree_slice] = multiConvMol.deg_slice\n",
    "      d[membership] = multiConvMol.membership\n",
    "      for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "        d[deg_adjs[i - 1]] = multiConvMol.get_deg_adjacency_lists()[i]\n",
    "      yield d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model using `TensorGraph.fit_generator(generator)` which will use the generator we've defined to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Ending global_step 251: Average loss 530.84\n",
      "TIMING: model fitting took 6.949 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "530.8396410260882"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epochs set to 1 to render tutorials online.\n",
    "# Set epochs=10 for better results.\n",
    "tg.fit_generator(data_generator(train_dataset, epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our graph convolutional method, let's evaluate its performance. We again have to use our defined generator to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.83463194036351052, 0.86218739964675661, 0.84894031662657832, 0.80217986671584707, 0.70559942152332189, 0.79751934844253025, 0.8103057689046107, 0.71659210162938414, 0.80849247997327445, 0.72071717294380933, 0.83433314746710274, 0.78304357554399506]\n",
      "Training ROC-AUC Score: 0.793712\n",
      "computed_metrics: [0.78221936377578793, 0.78993055555555547, 0.81705388431256543, 0.77777071682765631, 0.66802272727272727, 0.67197702777122181, 0.64295604015230179, 0.72305596655628368, 0.74692724275959499, 0.63050611290902547, 0.80023473616134511, 0.73880275624461667]\n",
      "Valid ROC-AUC Score: 0.732455\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "def reshape_y_pred(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TensorGraph.Predict returns a list of arrays, one for each output\n",
    "    We also have to remove the padding on the last batch\n",
    "    Metrics taks results of shape (samples, n_task, prob_of_class)\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    retval = np.stack(y_pred, axis=1)\n",
    "    return retval[:n_samples]\n",
    "    \n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_predictions = tg.predict_on_generator(data_generator(train_dataset, predict=True))\n",
    "train_predictions = reshape_y_pred(train_dataset.y, train_predictions)\n",
    "train_scores = metric.compute_metric(train_dataset.y, train_predictions, train_dataset.w)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores)\n",
    "\n",
    "valid_predictions = tg.predict_on_generator(data_generator(valid_dataset, predict=True))\n",
    "valid_predictions = reshape_y_pred(valid_dataset.y, valid_predictions)\n",
    "valid_scores = metric.compute_metric(valid_dataset.y, valid_predictions, valid_dataset.w)\n",
    "print(\"Valid ROC-AUC Score: %f\" % valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The model we've constructed behaves nearly identically to `GraphConvModel`. If you're looking to build your own custom models, you can follow the example we've provided here to do so. We hope to see exciting constructions from your end soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Congratulations! Time to join the Community!\n",
    "\n",
    "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
    "\n",
    "## Star DeepChem on GitHub\n",
    "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
    "\n",
    "## Join the DeepChem Gitter\n",
    "The DeepChem [Gitter](https://gitter.im/deepchem/Lobby) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
