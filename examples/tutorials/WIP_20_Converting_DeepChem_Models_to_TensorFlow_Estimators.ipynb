{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "WIP_20_Converting_DeepChem_Models_to_TensorFlow_Estimators.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6EyTq_tQXhw",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial Part 20: Converting DeepChem models to TensorFlow Estimators\n",
        "\n",
        "So far, we've walked through a lot of the scientific details tied to molecular machine learning, but we haven't discussed as much how to use tools like DeepChem in production settings. This tutorial (and the last) focus more on the practical matters of how to use DeepChem in production settings.\n",
        "\n",
        "When DeepChem was first created, Tensorflow had no standard interface for datasets or models.  We created the Dataset and Model classes to fill this hole.  More recently, Tensorflow has added the `tf.data` module as a standard interface for datasets, and the `tf.estimator` module as a standard interface for models.  To enable easy interoperability with other tools, we have added features to Dataset and Model to support these new standards. Using the Estimator interface may make it easier to deply DeepChem models in production environments.\n",
        "\n",
        "This example demonstrates how to use these features.  Let's begin by loading a dataset and creating a model to analyze it.  We'll use a simple MultitaskClassifier with one hidden layer.\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/20_Converting_DeepChem_Models_to_TensorFlow_Estimators.ipynb)\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run DeepChem within Colab, you'll need to run the following cell of installation commands. This will take about 5 minutes to run to completion and install your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh09-nheQXh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%tensorflow_version 1.x\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.3.0\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM8uHD_fQXh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "861758e7-9dd2-4fe4-f71d-608254d64544"
      },
      "source": [
        "import deepchem as dc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tasks, datasets, transformers = dc.molnet.load_tox21(reload=False)\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "n_tasks = len(tasks)\n",
        "n_features = train_dataset.X.shape[1]\n",
        "\n",
        "model = dc.models.MultitaskClassifier(n_tasks, n_features, layer_sizes=[1000], dropouts=0.25)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Loading raw samples now.\n",
            "shard_size: 8192\n",
            "About to start loading CSV from /tmp/tox21.csv.gz\n",
            "Loading shard 1 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "TIMING: featurizing shard 0 took 30.295 s\n",
            "TIMING: dataset construction took 30.661 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.427 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.203 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.200 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.341 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.048 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 0.047 s\n",
            "Loading dataset from disk.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuKIZtbUQXiE",
        "colab_type": "text"
      },
      "source": [
        "We want to train the model using the training set, then evaluate it on the test set.  As our evaluation metric we will use the ROC AUC, averaged over the 12 tasks included in the dataset.  First let's see how to do this with the DeepChem API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5zUpjFlQXiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "e34f87c8-ae6e-44ac-837c-431cdb668753"
      },
      "source": [
        "model.fit(train_dataset, nb_epoch=10)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
        "print(model.evaluate(test_dataset, [metric]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/keras_model.py:169: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/optimizers.py:76: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/keras_model.py:258: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/keras_model.py:260: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/keras_model.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/losses.py:108: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/deepchem/models/losses.py:109: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "computed_metrics: [0.7718502121379819, 0.8131287585005338, 0.8541209084124107, 0.8087711953108645, 0.7040240528116082, 0.7804721388054721, 0.663604766633565, 0.6614564551027767, 0.8348617621085523, 0.71728740936058, 0.8380653475224475, 0.7039911308203991]\n",
            "{'mean-roc_auc_score': 0.7626361781272659}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXcyNGQXQXiN",
        "colab_type": "text"
      },
      "source": [
        "Simple enough.  Now let's see how to do the same thing with the Tensorflow APIs.  Fair warning: this is going to take a lot more code!\n",
        "\n",
        "To begin with, Tensorflow doesn't allow a dataset to be passed directly to a model.  Instead, you need to write an \"input function\" to construct a particular set of tensors and return them in a particular format.  Fortunately, Dataset's `make_iterator()` method provides exactly the tensors we need in the form of a `tf.data.Iterator`.  This allows our input function to be very simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjGKtwReQXiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn(dataset, epochs):\n",
        "    x, y, weights = dataset.make_iterator(batch_size=100, epochs=epochs).get_next()\n",
        "    return {'x': x, 'weights': weights}, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_KL9OinQXiS",
        "colab_type": "text"
      },
      "source": [
        "Next, you have to use the functions in the `tf.feature_column` module to create an object representing each feature and weight column (but curiously, *not* the label column—don't ask me why!).  These objects describe the data type and shape of each column, and give each one a name.  The names must match the keys in the dict returned by the input function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57-Yl90SQXiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_col = tf.feature_column.numeric_column('x', shape=(n_features,))\n",
        "weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weVnAymyQXid",
        "colab_type": "text"
      },
      "source": [
        "Unlike DeepChem models, which allow arbitrary metrics to be passed to `evaluate()`, estimators require all metrics to be defined up front when you create the estimator.  Unfortunately, Tensorflow doesn't have very good support for multitask models.  It provides an AUC metric, but no easy way to average this metric over tasks.  We therefore must create a separate metric for every task, then define our own metric function to compute the average of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCckYybyQXie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_auc(labels, predictions, weights):\n",
        "    metric_ops = []\n",
        "    update_ops = []\n",
        "    for i in range(n_tasks):\n",
        "        metric, update = tf.metrics.auc(labels[:,i], predictions[:,i], weights[:,i])\n",
        "        metric_ops.append(metric)\n",
        "        update_ops.append(update)\n",
        "    mean_metric = tf.reduce_mean(tf.stack(metric_ops))\n",
        "    update_all = tf.group(*update_ops)\n",
        "    return mean_metric, update_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suikbE_FQXii",
        "colab_type": "text"
      },
      "source": [
        "Now we create our `Estimator` by calling `make_estimator()` on the DeepChem model.  We provide as arguments the objects created above to represent the feature and weight columns, as well as our metric function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUR_q5ugQXij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "d805b23b-12d7-4a31-c5c8-7ae96bb25cd9"
      },
      "source": [
        "#estimator = model.make_estimator(feature_columns=[x_col],\n",
        "#                                 weight_column=weight_col,\n",
        "#                                 metrics={'mean_auc': mean_auc},\n",
        "#                                 model_dir='estimator')\n",
        "estimator = tf.keras.estimator.model_to_estimator(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpr61vv2x9\n",
            "INFO:tensorflow:Using the Keras model provided.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6cc0c716ca8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/estimator/__init__.py\u001b[0m in \u001b[0;36mmodel_to_estimator\u001b[0;34m(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mcheckpoint_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m       use_v2_estimator=False)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/keras.py\u001b[0m in \u001b[0;36mmodel_to_estimator\u001b[0;34m(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format, use_v2_estimator)\u001b[0m\n\u001b[1;32m    558\u001b[0m   keras_model_fn = _create_keras_model_fn(keras_model, custom_objects,\n\u001b[1;32m    559\u001b[0m                                           save_object_ckpt)\n\u001b[0;32m--> 560\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_any_weight_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;31m# Warn if config passed to estimator tries to update GPUOptions. If a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# session has already been created, the GPUOptions passed to the first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/keras.py\u001b[0m in \u001b[0;36m_any_weight_initialized\u001b[0;34m(keras_model)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MultitaskClassifier' object has no attribute 'layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxhP2VVTQXiq",
        "colab_type": "text"
      },
      "source": [
        "We are finally ready to train and evaluate it!  Notice how the input function passed to each method is actually a lambda.  This allows us to write a single function, then use it with different datasets and numbers of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnzpHwgcQXis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=lambda: input_fn(train_dataset, 100))\n",
        "print(estimator.evaluate(input_fn=lambda: input_fn(test_dataset, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qai7_prqQXiw",
        "colab_type": "text"
      },
      "source": [
        "That's a lot of code for something DeepChem can do in three lines.  The Tensorflow API is verbose and somewhat confusing.  It has seemingly arbitrary limitations, like assuming a model will only ever have one output, and therefore only allowing one label.  But for better or worse, it's a standard.\n",
        "\n",
        "Of course, if you just want to use a DeepChem model with a DeepChem dataset, there is no need for any of this.  Just use the DeepChem API.  But perhaps you want to use a DeepChem dataset with a model that has been implemented as an estimator.  In that case, `Dataset.make_iterator()` allows you to easily do that.  Or perhaps you have higher level workflow code that is written to work with estimators.  In that case, `make_estimator()` allows DeepChem models to easily fit into that workflow."
      ]
    }
  ]
}