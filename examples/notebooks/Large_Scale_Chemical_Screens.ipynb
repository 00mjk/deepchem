{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screening Zinc For HIV Inhibition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial I will walk through how to efficiently screen a large compound library with DeepChem (ZINC).  Screening a large compound library using machine learning is a CPU bound pleasingly parrellel problem.  The actual code examples I will use assume the resources available are a single very big machine (like an AWS c5.18xlarge), but should be readily swappable for other systmes (like a super computing cluster).  At a high level what we will do is...\n",
    "\n",
    "1. Create a Machine Learning Model Over Labeled Data\n",
    "2. Transform ZINC into \"Work-Units\"\n",
    "3. Create an inference script which runs predictions over a \"Work-Unit\"\n",
    "4. Load \"Work-Unit\" into \"distribution mechanism\"\n",
    "5. Consume work units from \"distribution mechanism\"\n",
    "6. Gather Results\n",
    "\n",
    "# 1. Train Model On Labelled Data\n",
    "\n",
    "We are just going to knock out a simple model here.  In a real world problem you will probably try several models and do a little hyper parameter searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.molnet.load_function import hiv_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leswing/miniconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.7494862112294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchem.models import GraphConvModel\n",
    "from deepchem.data import NumpyDataset\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "tasks, all_datasets, transformers = hiv_datasets.load_hiv(featurizer=\"GraphConv\")\n",
    "train, valid, test = [NumpyDataset.from_DiskDataset(x) for x in all_datasets]\n",
    "model = GraphConvModel(1, mode=\"classification\")\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score:0.22277598937482013\n",
      "Hit Rate Top 100: 0.33\n"
     ]
    }
   ],
   "source": [
    "y_true = np.squeeze(valid.y)\n",
    "y_pred = model.predict(valid)[:,0,1]\n",
    "print(\"Average Precision Score:%s\" % average_precision_score(y_true, y_pred))\n",
    "sorted_results = sorted(zip(y_pred, y_true), reverse=True)\n",
    "hit_rate_100 = sum(x[1] for x in sorted_results[:100]) / 100\n",
    "print(\"Hit Rate Top 100: %s\" % hit_rate_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Model Over Full Dataset For The Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.701 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 15.869 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 19.106 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 16.267 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 16.754 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.446 s\n",
      "TIMING: dataset construction took 98.214 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 21.127 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leswing/miniconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tasks, all_datasets, transformers = hiv_datasets.load_hiv(featurizer=\"GraphConv\", split=None)\n",
    "\n",
    "model = GraphConvModel(1, mode=\"classification\", model_dir=\"/tmp/zinc/screen_model\")\n",
    "model.fit(all_datasets[0])\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Work-Units\n",
    "\n",
    "1. Download All of ZINC15.\n",
    "\n",
    "Go to http://zinc15.docking.org/tranches/home and download all non-empty tranches in .smi format.\n",
    "I found it easiest to download the wget script and then run the wget script.\n",
    "For the rest of this tutorial I will assume zinc was downloaded to /tmp/zinc.\n",
    "\n",
    "\n",
    "The way zinc downloads the data isn't great for inference.  We want \"Work-Units\" which a single CPU can execute that takes a resonable amount of time (10 minutes to an hour).  To accomplish this we are going to split the zinc data into files each with 500 thousand lines.\n",
    "\n",
    "\n",
    "```bash\n",
    "mkdir /tmp/zinc/screen\n",
    "find /tmp/zinc -name '*.smi' -exec cat {} \\; | grep -iv \"smiles\" \\\n",
    "     | split -l 500000 /tmp/zinc/screen/segment\n",
    "```\n",
    "\n",
    "This bash command\n",
    "1. Finds all smi files\n",
    "2. prints to stdout the contents of the file\n",
    "3. removes header lines\n",
    "4. splits into multiple files in /tmp/zinc/screen that are 1 million molecules long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creat Inference Script\n",
    "\n",
    "Now that we have work unit we need to construct a program which ingests a work unit and logs the result.\n",
    "For this example we will get the work unit via a file-path, and log the result to a file.\n",
    "An easy extensions to distribute over multiple computers would be to get the work unit via a url, and log the results to a distributed queue.\n",
    "\n",
    "Here is what mine looks like\n",
    "\n",
    "inference.py\n",
    "```python\n",
    "import deepchem as dc\n",
    "\n",
    "def create_dataset(lines, batch_size=50000):\n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        chunk = lines[i:i+batch_size]\n",
    "        mols, orig_lines = [], []\n",
    "        for line in chunk:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(line[0])\n",
    "                if mol is None:\n",
    "                    continue\n",
    "                orig_lines.append(line)\n",
    "                mols.append(mol)\n",
    "            except:\n",
    "                pass\n",
    "        features = featurizer.featurize(mols)\n",
    "        ds = dc.data.NumpyDataset(features, np.ones(len(features))\n",
    "        yield ds, orig_lines\n",
    "        \n",
    "    \n",
    "\n",
    "def evaluate(fname):\n",
    "    lines = [x.strip().split() for x in open(fname).readlines()]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(sys.argv[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load \"Work-Unit\" into \"distribution mechanism\"\n",
    "\n",
    "We are going to use a flat file as our distribution mechanism.  It will be a bash script calling our inference script for every work unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "work_units = os.listdir('/tmp/zinc/screen')\n",
    "with open('/tmp/zinc/driver.sh', 'w') as fout:\n",
    "    fout.write(\"#!/bin/bash\\n\")\n",
    "    fout.write(\"export PATH=%s\" % os.environ[\"PATH\"])\n",
    "    for work_unit in work_units:\n",
    "        full_path = os.path.join('/tmp/zinc', work_unit)\n",
    "        fout.write(\"python inference.py %s\" % full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Consume work units from \"distribution mechanism\"\n",
    "\n",
    "We will consume work units from our flat file using a very simple process_pool.  It takes lines from our \"distribution mechanism\" and runs them, running as many processes in parrallel as we have cpus.\n",
    "\n",
    "process_pool.py\n",
    "```python\n",
    "import multiprocessing\n",
    "import sys\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import delegator\n",
    "\n",
    "\n",
    "def run_command(args):\n",
    "  q, command = args\n",
    "  cpu_id = q.get()\n",
    "  try:\n",
    "    command = \"taskset -c %s %s\" % (cpu_id, command)\n",
    "    print(\"running %s\" % command)\n",
    "    c = delegator.run(command)\n",
    "    print(c.err)\n",
    "    print(c.out)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "  q.put(cpu_id)\n",
    "\n",
    "\n",
    "def main(n_processors, command_file):\n",
    "  commands = [x.strip() for x in open(command_file).readlines()]\n",
    "  commands = list(filter(lambda x: not x.startswith(\"#\"), commands))\n",
    "  q = multiprocessing.Manager().Queue()\n",
    "  for i in range(n_processors):\n",
    "    q.put(i)\n",
    "  argslist = [(q, x) for x in commands]\n",
    "  pool = Pool(processes=n_processors)\n",
    "  pool.map(run_command, argslist)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  processors = multiprocessing.cpu_count()\n",
    "  main(processors, sys.argv[1])\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    ">> python process_pool.py /tmp/zinc/driver.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gather Results\n",
    "Since we logged our results to \\*_out.smi we now need to gather all of them up and sort them by our predictions\n",
    "\n",
    "```bash\n",
    "find /tmp/zinc -name '*_out.smi' -exec cat {} \\; | sort -rn -k 3,3 > /tmp/zinc/screen/sorted_results.smi\n",
    "# Put the top 100k scoring molecules in their own file\n",
    "head -n 50000 /tmp/zinc/screen/sorted_results. > /tmp/zinc/screen/top_100k.smi\n",
    "```\n",
    "\n",
    "/tmp/zinc/screen/top_100k.smi is now a small enough file to investigate using standard tools like pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
