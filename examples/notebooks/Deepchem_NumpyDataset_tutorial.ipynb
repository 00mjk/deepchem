{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deepchem Datasets\n",
    "In this tutorial we will have a look at various deepchem `dataset` methods present in `deepchem.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "RDKit WARNING: [17:49:31] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NumpyDatasets \n",
    "\n",
    "The `dc.data.NumpyDatasets` class is used when you have your data in numpy arrays. It provides a simple wrapper around a collection of Numpy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is your dataset in numpy array of size : 20x20.\n",
    "data = np.random.random((4, 4))\n",
    "labels = np.random.random((4,)) # labels of size 20x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.data.datasets import NumpyDataset # import NumpyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepchem.data.datasets.NumpyDataset at 0x1a3a047a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NumpyDataset(data, labels) # creates numpy dataset object\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting X, y from NumpyDataset Object\n",
    "Extracting the data and labels from the NumpyDataset is very easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77747145, 0.83258316, 0.76509785, 0.36074566],\n",
       "       [0.28224673, 0.79519759, 0.93776705, 0.2213494 ],\n",
       "       [0.54740751, 0.38403327, 0.12592795, 0.94350571],\n",
       "       [0.02717497, 0.75938816, 0.9477633 , 0.80792975]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X # Extracts the data (X) from the NumpyDataset Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76098746, 0.12423036, 0.24516253, 0.84793405])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y # Extracts the labels (y) from the NumpyDataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights of a dataset - w\n",
    "So apart from `X` and `y` which are the data and the labels, you can also assign weights `w` to each data instance. The dimension of `w` is same as that of `y`(which is `Nx1` where `N` is the number of data instances).\n",
    "\n",
    "**NOTE:** By default `w` is a vector initialized with equal weights (all being 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.w # printing the weights that are assigned by default. Notice that they are a vector of 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random((4,)) # initializing weights with random vector of size 20x1\n",
    "dataset_with_weights = NumpyDataset(data, labels, w) # creates numpy dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10909932, 0.54252096, 0.70115951, 0.39749864])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_weights.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over NumpyDataset\n",
    "In order to iterate over NumpyDataset, we use `itersamples` method. We iterate over 4 quantities, namely `X`, `y`, `w` and `ids`. The first three quantities are the same as discussed above and `ids` is the id of the data instance. By default the id is given in order starting from `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77747145 0.83258316 0.76509785 0.36074566] 0.7609874556128873 1.0 0\n",
      "[0.28224673 0.79519759 0.93776705 0.2213494 ] 0.1242303578243128 1.0 1\n",
      "[0.54740751 0.38403327 0.12592795 0.94350571] 0.2451625327575474 1.0 2\n",
      "[0.02717497 0.75938816 0.9477633  0.80792975] 0.8479340478005098 1.0 3\n"
     ]
    }
   ],
   "source": [
    "for x, y, w, id in dataset.itersamples():\n",
    "    print(x, y, w, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract the ids by `dataset.ids`. This would return a numpy array consisting of the ids of the data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Example\n",
    "Just to get a better understanding, lets take read MNIST data and use `NumpyDataset` to store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy data of MNIST into NumpyDataset\n",
    "train = NumpyDataset(mnist.train.images, mnist.train.labels)\n",
    "valid = NumpyDataset(mnist.validation.images, mnist.validation.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAADzpJREFUeJzt3X+QVfV5x/HPA66AqInEigQxiGIVHUVmQVvbhAzFGsGgyUhlOgmdSd0ko5naIW0cpmn8pw2TiaIxqQaVBEfjj6mgJGLUoVZjo9RV8QdBxTFrQBYQ8Qda5cfu0z/2kFlxz/de7j33ngvP+zXj7L3nOT+e3PDZc+9+7zlfc3cBiGdQ2Q0AKAfhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1EHNPNjBNsSHangzDwmE8qHe107fYdWsW1f4zexcSddKGizpJndfkFp/qIbrTJtWzyEBJKzylVWvW/PbfjMbLOknkr4gaYKkOWY2odb9AWiuej7zT5H0iru/6u47Jd0haVYxbQFotHrCP1rS+n7PN2TLPsLMOsys08w6d2lHHYcDUKR6wj/QHxU+dn2wuy9y93Z3b2/TkDoOB6BI9YR/g6Qx/Z4fI2ljfe0AaJZ6wv+kpPFmdpyZHSzpYknLi2kLQKPVPNTn7rvN7DJJD6hvqG+xu68prDMADVXXOL+7r5C0oqBeADQRX+8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqLpm6TWzLknbJfVI2u3u7UU0hf3HjhmTk/Vtl7yXW3tm8m1Ft/MR39jwl7m1x+4/PbntuJ++mqzv7t5UU0+tpK7wZz7v7lsL2A+AJuJtPxBUveF3SQ+a2VNm1lFEQwCao963/We7+0YzO0rSQ2b2ors/2n+F7JdChyQN1SF1Hg5AUeo687v7xuznFknLJE0ZYJ1F7t7u7u1tGlLP4QAUqObwm9lwMztsz2NJ50h6oajGADRWPW/7R0paZmZ79vMLd/91IV0BaDhz96Yd7HAb4WfatKYdD5VZ28HJ+stXn5Gs33f+wmT9hLbyPuoNkuXWepX+dz/xia8m68d8eU1NPTXaKl+pd31b/v/wfhjqA4Ii/EBQhB8IivADQRF+ICjCDwRVxFV92I+9dN3EZP3l8/8jWR+kocl6pSG1enSsn5qs3zTmkZr3/aOJdyTrV33qc8l6z5vbaj52s3DmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOc/AKQuy600jr9m5o8r7H1wstrd83/J+meXfTu3Nm7ZzuS2Q9alb4/ds/XNZP2MO/82t/bU5FuT2z79wdhk3XfuStb3B5z5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAoxvkPAN2X5s+M/vL511XYOj2Of/M7xybrSy+ZnqyP/58nKhw/3+6at+yzY0dbzdv+8vXTkvVh239f875bBWd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq4ji/mS2WNFPSFnc/NVs2QtKdksZK6pI0293falybSPlmx725tdQ01ZL0/TcnJOuPf/HEZN26Vifr9Rh8+OHJ+oa/PzVZ/+fTlubWntnZm9x22F/v/+P4lVRz5v+5pHP3WnaFpJXuPl7Syuw5gP1IxfC7+6OS9p5+ZJakJdnjJZIuKLgvAA1W62f+ke7eLUnZz6OKawlAMzT8u/1m1iGpQ5KG6pBGHw5AlWo98282s1GSlP3ckreiuy9y93Z3b2/TkBoPB6BotYZ/uaS52eO5kvL/3AygJVUMv5ndLulxSX9qZhvM7GuSFkiabmbrJE3PngPYj1T8zO/uc3JK0wruBTXqSfwO75Unt13x71OT9cO6ar8eX5I0KP9+AT2fOz256cwfr0zWv/HJh9OHTnzHYcZLlQaoXq9Q3//xDT8gKMIPBEX4gaAIPxAU4QeCIvxAUNy6O7hDNqWnya5Xajjv/ltvbOixL3zlvNzaoC+npxbvKbqZFsSZHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpz/ALDug5H5xU90JbddfMuPkvUFm/8qWf/v105I1n89JbX/Yclt3+n9MFmffN8/JusnzVuTW+t9//3kthFw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMw9fWvnIh1uI/xM447fhTvrtNzSr+7+WUMPXWkK8Eq3Dk+ZdO23kvVP/+C3Ne/7QLXKV+pd35b+PyXDmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqp4Pb+ZLZY0U9IWdz81W3alpEskvZGtNt/dVzSqyeh2zJicrK+/eHdurdI4fL0GW4Xzh/fmlqat+VJyU8bxG6uaM//PJZ07wPKF7j4x+4/gA/uZiuF390clbWtCLwCaqJ7P/JeZ2XNmttjMjiisIwBNUWv4r5d0vKSJkrolXZW3opl1mFmnmXXu0o4aDwegaDWF3903u3uPu/dKulHSlMS6i9y93d3b2zSk1j4BFKym8JvZqH5PL5T0QjHtAGiWaob6bpc0VdKRZrZB0vckTTWziZJcUpekrzewRwANwPX8TTDotJOS9aMXvZ6s3zTmkWS9nmvmK7liU/o7Bkv/tz1Zv376ktza+LY3k9t+9Z++nawfetcTyXpEXM8PoCLCDwRF+IGgCD8QFOEHgiL8QFAM9RVga8efJesPfPeHyfonBg1N1uu5Pfa87rOS297/X+mhuhMX/j5Z3929KVnv+fyk/GPfemNy2xveHpes/+oULinZG0N9ACoi/EBQhB8IivADQRF+ICjCDwRF+IGgKl7Pjz7bL84fL693HH/trl3J+sJN05P1l645Jf/Y96xObjvuw8eT9fybgldn8CPP5tZOuuvS5LbPXnRNsr7snMuS9bYHO5P16DjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNXaetp+ZdIVxrHX/b+iGT9Z7NnJOu9q3+XrB+m/FtY50+Q3RyDhuW/NqdM6kpuO8TakvXegxo7/fiBjjM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVcZzfzMZIukXS0eobNl7k7tea2QhJd0oaK6lL0mx3f6txrbauSvfV/87Ds5P1E1c/WWQ7TTX4yE8l64csy39t7hy3osLeGcdvpGrO/LslzXP3kyWdJelSM5sg6QpJK919vKSV2XMA+4mK4Xf3bnd/Onu8XdJaSaMlzZK0JFttiaQLGtUkgOLt02d+Mxsr6QxJqySNdPduqe8XhKSjim4OQONUHX4zO1TS3ZIud/d392G7DjPrNLPOXdpRS48AGqCq8JtZm/qCf5u7L80WbzazUVl9lKQtA23r7ovcvd3d29s0pIieARSgYvjNzCTdLGmtu1/dr7Rc0tzs8VxJ9xbfHoBGqeaS3rMlfUXS82a25z7Q8yUtkHSXmX1N0h8kXdSYFlvDkc/lT4P9Vu8HyW2fPC99C+rJP708WT/5X19L1ns2D/imqyoHjf50sv7+6aOT9cuvvT1Zn3HIO7m1Spcb/+Tt45P1Yb95MVkv+3LmVlcx/O7+mPIHXKcV2w6AZuEbfkBQhB8IivADQRF+ICjCDwRF+IGgzD1//Lpoh9sIP9MOvNHB9f/y58n6s9+8rq79r9mZnij78nV/U/O+//Pk25L1Srclr3Q5c6/y/33N686f9lySXvzWhGTdHs+f/juqVb5S7/q2qq6F5swPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ExRXcBRrzYk6zf8Pa4ZH3C0A3J+tSh6WHbh065O1lPS4/jV3LDO59J1hfeNzO3Nv67zyS3tQ8Zx28kzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTX87eAg8Yem6yvW/DJmvf9/Un3JOu/3X5Csv7LB85M1o+b//g+94TG4Xp+ABURfiAowg8ERfiBoAg/EBThB4Ii/EBQFcf5zWyMpFskHa2+Kc8Xufu1ZnalpEskvZGtOt/dV6T2xTg/0Fj7Ms5fzc08dkua5+5Pm9lhkp4ys4ey2kJ3/2GtjQIoT8Xwu3u3pO7s8XYzWytpdKMbA9BY+/SZ38zGSjpD0qps0WVm9pyZLTazI3K26TCzTjPr3KUddTULoDhVh9/MDpV0t6TL3f1dSddLOl7SRPW9M7hqoO3cfZG7t7t7e5uGFNAygCJUFX4za1Nf8G9z96WS5O6b3b3H3Xsl3ShpSuPaBFC0iuE3M5N0s6S17n51v+Wj+q12oaQXim8PQKNU89f+syV9RdLzZrY6WzZf0hwzmyjJJXVJ+npDOgTQENX8tf8xacBJ2JNj+gBaG9/wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBNXUKbrN7A1Jr/VbdKSkrU1rYN+0am+t2pdEb7UqsrfPuPufVLNiU8P/sYObdbp7e2kNJLRqb63al0RvtSqrN972A0ERfiCossO/qOTjp7Rqb63al0RvtSqlt1I/8wMoT9lnfgAlKSX8Znaumb1kZq+Y2RVl9JDHzLrM7HkzW21mnSX3stjMtpjZC/2WjTCzh8xsXfZzwGnSSurtSjN7PXvtVpvZeSX1NsbMHjaztWa2xsz+IVte6muX6KuU163pb/vNbLCklyVNl7RB0pOS5rj775raSA4z65LU7u6ljwmb2WclvSfpFnc/NVv2A0nb3H1B9ovzCHf/Tov0dqWk98qeuTmbUGZU/5mlJV0g6e9U4muX6Gu2SnjdyjjzT5H0iru/6u47Jd0haVYJfbQ8d39U0ra9Fs+StCR7vER9/3iaLqe3luDu3e7+dPZ4u6Q9M0uX+tol+ipFGeEfLWl9v+cb1FpTfrukB83sKTPrKLuZAYzMpk3fM336USX3s7eKMzc3014zS7fMa1fLjNdFKyP8A83+00pDDme7+yRJX5B0afb2FtWpaubmZhlgZumWUOuM10UrI/wbJI3p9/wYSRtL6GNA7r4x+7lF0jK13uzDm/dMkpr93FJyP3/USjM3DzSztFrgtWulGa/LCP+Tksab2XFmdrCkiyUtL6GPjzGz4dkfYmRmwyWdo9abfXi5pLnZ47mS7i2xl49olZmb82aWVsmvXavNeF3Kl3yyoYxrJA2WtNjd/63pTQzAzMap72wv9U1i+osyezOz2yVNVd9VX5slfU/SPZLuknSspD9Iusjdm/6Ht5zepqrvresfZ27e8xm7yb39haTfSHpeUm+2eL76Pl+X9tol+pqjEl43vuEHBMU3/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPX/0dpafaOuFKsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize one sample \n",
    "sample = np.reshape(train.X[5], (28, 28))\n",
    "plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a Numpy Array to tf.data.dataset()\n",
    "\n",
    "\n",
    "Let's say you want to use the `tf.data` module instead of DeepChem's data handling library. Doing this is straightforward and is quite similar to getting a `NumpyDataset` object from numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "\n",
      "[[0.09102272 0.07158817 0.85294433 0.72889589 0.00564065]\n",
      " [0.26971883 0.51840485 0.69322473 0.85085169 0.11202028]\n",
      " [0.14868434 0.83661216 0.32333968 0.64312229 0.44279518]\n",
      " [0.15123109 0.3443811  0.04610284 0.66125549 0.26025301]]\n",
      "\n",
      " Labels\n",
      "[0.75613112 0.97179618 0.33262846 0.54677704]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_small = np.random.random((4,5))\n",
    "label_small = np.random.random((4,))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_small, label_small))\n",
    "print (\"Data\\n\")\n",
    "print (data_small)\n",
    "print (\"\\n Labels\")\n",
    "print (label_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the numpy dataset from tf.data\n",
    "\n",
    "In order to extract the numpy array from the `tf.data`, you first need to define an `iterator` to iterate over the `tf.data.Dataset` object and then in the tensorflow session, run over the iterator to get the data instances. Let's have a look at how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-f67e6d094179>:1: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Numpy Data\n",
      "[[0.09102272 0.07158817 0.85294433 0.72889589 0.00564065]\n",
      " [0.26971883 0.51840485 0.69322473 0.85085169 0.11202028]\n",
      " [0.14868434 0.83661216 0.32333968 0.64312229 0.44279518]\n",
      " [0.15123109 0.3443811  0.04610284 0.66125549 0.26025301]]\n",
      "\n",
      " Numpy Label\n",
      "[0.75613112 0.97179618 0.33262846 0.54677704]\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator() # iterator\n",
    "next_element = iterator.get_next()\n",
    "numpy_data = np.zeros((4, 5))\n",
    "numpy_label = np.zeros((4,))\n",
    "sess = tf.Session() # tensorflow session \n",
    "for i in range(4):\n",
    "    data_, label_ = sess.run(next_element) # data_ contains the data and label_ contains the labels that we fed in the previous step\n",
    "    numpy_data[i, :] = data_\n",
    "    numpy_label[i] = label_\n",
    "    \n",
    "print (\"Numpy Data\")\n",
    "print(numpy_data)\n",
    "print (\"\\n Numpy Label\")\n",
    "print(numpy_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the numpy arrays of `data` and `labels`, you can convert it to `NumpyDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09102272, 0.07158817, 0.85294433, 0.72889589, 0.00564065],\n",
       "       [0.26971883, 0.51840485, 0.69322473, 0.85085169, 0.11202028],\n",
       "       [0.14868434, 0.83661216, 0.32333968, 0.64312229, 0.44279518],\n",
       "       [0.15123109, 0.3443811 , 0.04610284, 0.66125549, 0.26025301]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = NumpyDataset(numpy_data, numpy_label) # convert to NumpyDataset\n",
    "dataset_.X  # printing just to check if the data is same!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting NumpyDataset to `tf.data`\n",
    "This can be easily done by the `make_iterator()` method of `NumpyDataset`. This converts the `NumpyDataset` to `tf.data`. Let's look how it's done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Numpy Data\n",
      "[[0.26971883 0.51840485 0.69322473 0.85085169 0.11202028]\n",
      " [0.14868434 0.83661216 0.32333968 0.64312229 0.44279518]\n",
      " [0.09102272 0.07158817 0.85294433 0.72889589 0.00564065]\n",
      " [0.15123109 0.3443811  0.04610284 0.66125549 0.26025301]]\n",
      "\n",
      " Numpy Label\n",
      "[0.97179618 0.33262846 0.75613112 0.54677704]\n"
     ]
    }
   ],
   "source": [
    "iterator_ = dataset_.make_iterator() # Using make_iterator for converting NumpyDataset to tf.data\n",
    "next_element_ = iterator_.get_next()\n",
    "\n",
    "sess = tf.Session() # tensorflow session \n",
    "data_and_labels = sess.run(next_element_) # data_ contains the data and label_ contains the labels that we fed in the previous step\n",
    "\n",
    "\n",
    "print (\"Numpy Data\")\n",
    "print(data_and_labels[0])  # Data in the first index \n",
    "print (\"\\n Numpy Label\")\n",
    "print(data_and_labels[1])  # Labels in the second index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
