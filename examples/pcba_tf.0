Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.042 s
TIMING: convert_df_to_numpy missing elts computation took 0.394 s
TIMING: convert_df_to_numpy took 1.671 s
TIMING: writing metadata row took 3.446 s
TIMING: shard featurization took 60.581 s
TIMING: featurization map function took 60.581 s
Loading shard 43 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.037 s
TIMING: convert_df_to_numpy missing elts computation took 0.389 s
TIMING: convert_df_to_numpy took 1.660 s
TIMING: writing metadata row took 3.419 s
TIMING: shard featurization took 60.773 s
TIMING: featurization map function took 60.773 s
Loading shard 44 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.036 s
TIMING: convert_df_to_numpy x computation took 1.029 s
TIMING: convert_df_to_numpy missing elts computation took 0.396 s
TIMING: convert_df_to_numpy took 1.658 s
TIMING: writing metadata row took 3.416 s
TIMING: shard featurization took 60.875 s
TIMING: featurization map function took 60.875 s
Loading shard 45 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.036 s
TIMING: convert_df_to_numpy missing elts computation took 0.393 s
TIMING: convert_df_to_numpy took 1.665 s
TIMING: writing metadata row took 3.431 s
TIMING: shard featurization took 60.814 s
TIMING: featurization map function took 60.814 s
Loading shard 46 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.033 s
TIMING: convert_df_to_numpy missing elts computation took 0.396 s
TIMING: convert_df_to_numpy took 1.665 s
TIMING: writing metadata row took 3.429 s
TIMING: shard featurization took 60.779 s
TIMING: featurization map function took 60.779 s
Loading shard 47 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.033 s
TIMING: convert_df_to_numpy missing elts computation took 0.394 s
TIMING: convert_df_to_numpy took 1.662 s
TIMING: writing metadata row took 3.425 s
TIMING: shard featurization took 60.613 s
TIMING: featurization map function took 60.613 s
Loading shard 48 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.036 s
TIMING: convert_df_to_numpy x computation took 1.041 s
TIMING: convert_df_to_numpy missing elts computation took 0.395 s
TIMING: convert_df_to_numpy took 1.671 s
TIMING: writing metadata row took 3.434 s
TIMING: shard featurization took 60.825 s
TIMING: featurization map function took 60.825 s
TIMING: map call on batch took 1476.498 s
Featurized 393216 datapoints

About to start processing next batch of shards
Loading shard 49 of size 8192.
Loading shard 50 of size 8192.
Loading shard 51 of size 8192.
Loading shard 52 of size 8192.
Loading shard 53 of size 8192.
Loading shard 54 of size 8192.
Loading shard 49 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.041 s
TIMING: convert_df_to_numpy x computation took 1.053 s
TIMING: convert_df_to_numpy missing elts computation took 0.392 s
TIMING: convert_df_to_numpy took 1.694 s
TIMING: writing metadata row took 3.477 s
TIMING: shard featurization took 60.938 s
TIMING: featurization map function took 60.938 s
Loading shard 50 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.053 s
TIMING: convert_df_to_numpy missing elts computation took 0.393 s
TIMING: convert_df_to_numpy took 1.685 s
TIMING: writing metadata row took 3.450 s
TIMING: shard featurization took 60.824 s
TIMING: featurization map function took 60.824 s
Loading shard 51 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.051 s
TIMING: convert_df_to_numpy missing elts computation took 0.390 s
TIMING: convert_df_to_numpy took 1.681 s
TIMING: writing metadata row took 3.447 s
TIMING: shard featurization took 60.838 s
TIMING: featurization map function took 60.838 s
Loading shard 52 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.037 s
TIMING: convert_df_to_numpy x computation took 1.048 s
TIMING: convert_df_to_numpy missing elts computation took 0.392 s
TIMING: convert_df_to_numpy took 1.679 s
TIMING: writing metadata row took 3.444 s
TIMING: shard featurization took 60.770 s
TIMING: featurization map function took 60.770 s
Loading shard 53 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
Featurizing sample 6000
Featurizing sample 7000
Featurizing sample 8000
TIMING: convert_df_to_numpy y computation took 0.040 s
TIMING: convert_df_to_numpy x computation took 1.055 s
TIMING: convert_df_to_numpy missing elts computation took 0.390 s
TIMING: convert_df_to_numpy took 1.694 s
TIMING: writing metadata row took 3.459 s
TIMING: shard featurization took 60.962 s
TIMING: featurization map function took 60.963 s
Loading shard 54 of size 8192 from file.
About to featurize shard.
Currently featurizing feature_type: CircularFingerprint
Featurizing sample 0
Featurizing sample 1000
Featurizing sample 2000
Featurizing sample 3000
Featurizing sample 4000
Featurizing sample 5000
TIMING: convert_df_to_numpy y computation took 0.029 s
TIMING: convert_df_to_numpy x computation took 0.689 s
TIMING: convert_df_to_numpy missing elts computation took 0.274 s
TIMING: convert_df_to_numpy took 1.126 s
TIMING: writing metadata row took 2.275 s
TIMING: shard featurization took 42.052 s
TIMING: featurization map function took 42.052 s
TIMING: map call on batch took 350.866 s
Featurized 589824 datapoints

About to start processing next batch of shards
TIMING: map call on batch took 0.000 s
TIMING: dataset construction took 0.052 s high
About to transform data
About to perform train/valid/test split.
Performing new split.
Computing train/valid/test indices
mean-roc_auc_score
-------------------------------------
Start fitting by tensorflow
Training for 10 epochs
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN Black
major: 3 minor: 5 memoryClockRate (GHz) 0.98
pciBusID 0000:85:00.0
Total memory: 5.94GiB
Free memory: 5.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:808] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:85:00.0)
On batch 0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 5388 get requests, put_count=5128 evicted_count=1000 eviction_rate=0.195008 and unsatisfied allocation rate=0.252413
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7184 get requests, put_count=7151 evicted_count=1000 eviction_rate=0.139841 and unsatisfied allocation rate=0.146993
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 0: Average loss 120.766
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 1: Average loss 97.5753
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 2: Average loss 72.9899
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 3: Average loss 80.7006
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
Constandse, Kayla Michelle
Akre, Alex Vivian
Cathers, Phillip Hamilton
Carrera, Bella Abigail
Davis, Christian KeShawn
McNiel, Chase Thomas
O'Keeffe, Fiona Rhiannon
Vesey, Annie Sinclair
Schaepe, Julia Marie
Cremers, Collin John
Rorick, Medora Evelyn
Cho, Young Joon
Elifas, Alex
Chiragzada, Selin
Anders, Rachel Sounthone
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 4: Average loss 80.2746
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 5: Average loss 96.8003
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 6: Average loss 96.6578
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 7: Average loss 97.3493
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 8: Average loss 92.2969
On batch 0
On batch 50
On batch 100
On batch 150
On batch 200
On batch 250
On batch 300
On batch 350
On batch 400
On batch 450
On batch 500
On batch 550
On batch 600
On batch 650
On batch 700
On batch 750
On batch 800
On batch 850
On batch 900
On batch 950
On batch 1000
On batch 1050
On batch 1100
On batch 1150
On batch 1200
On batch 1250
On batch 1300
On batch 1350
On batch 1400
On batch 1450
On batch 1500
On batch 1550
On batch 1600
On batch 1650
On batch 1700
On batch 1750
On batch 1800
On batch 1850
On batch 1900
On batch 1950
On batch 2000
On batch 2050
On batch 2100
On batch 2150
On batch 2200
On batch 2250
On batch 2300
On batch 2350
On batch 2400
On batch 2450
On batch 2500
On batch 2550
On batch 2600
On batch 2650
On batch 2700
On batch 2750
On batch 2800
On batch 2850
On batch 2900
On batch 2950
On batch 3000
On batch 3050
On batch 3100
On batch 3150
On batch 3200
On batch 3250
On batch 3300
On batch 3350
On batch 3400
On batch 3450
On batch 3500
On batch 3550
On batch 3600
On batch 3650
On batch 3700
On batch 3750
On batch 3800
On batch 3850
On batch 3900
On batch 3950
On batch 4000
On batch 4050
On batch 4100
On batch 4150
On batch 4200
On batch 4250
On batch 4300
On batch 4350
On batch 4400
On batch 4450
On batch 4500
On batch 4550
On batch 4600
On batch 4650
On batch 4700
On batch 4750
On batch 4800
On batch 4850
On batch 4900
On batch 4950
On batch 5000
On batch 5050
On batch 5100
On batch 5150
On batch 5200
On batch 5250
On batch 5300
On batch 5350
On batch 5400
On batch 5450
On batch 5500
Ending epoch 9: Average loss 92.5787
TIMING: model fitting took 11897.928 s high
I tensorflow/core/common_runtime/gpu/gpu_device.cc:808] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:85:00.0)
computed_metrics: [0.75002861653795283, 0.98819021503820403, 0.99069652196025992, 0.96928178860695757, 0.97021006944942156, 0.91280370948549638, 0.96178466747996461, 0.90845532217010572, 0.98205862365774399, 0.99025455116599415, 0.9812122099571815, 0.97214267651186004, 0.9677048921630822, 0.994216400279478, 0.88763879489380582, 0.98638516567197876, 0.98031388499726957, 0.98764423212691377, 0.89357589315665109, 0.98972413236123491, 0.93341761071958129, 0.94952403444683486, 0.98411832748743344, 0.9897090213051698, 0.8970379257410197, 0.97088421587599827, 0.90321321745127037, 0.97239537830350464, 0.99192107156976517, 0.9824061618120421, 0.95043740568260815, 0.93043292058421945, 0.99153040756892064, 0.97899130579435245, 0.99116734507896054, 0.94261793143651484, 0.94666784182384789, 0.93117642533029632, 0.91849959929870417, 0.97257286982156888, 0.97588508180792799, 0.96658656912899588, 0.8892597715720949, 0.98637654138163811, 0.99211742206160913, 0.95563634591779634, 0.98408412830427983, 0.79304121233331704, 0.88249242529023519, 0.86949490830386145, 0.8436381355253959, 0.94764200086943884, 0.85578098097121758, 0.99065335580097791, 0.9849019872003153, 0.99525424782158511, 0.94601786585104186, 0.99297809884485022, 0.87690717932186657, 0.97099463541443354, 0.9000216197619364, 0.94397691948367757, 0.99737385120421018, 0.98372028734560146, 0.93671445051768742, 0.95511823841866095, 0.99049482490738427, 0.92498459318808113, 0.98760549497936301, 0.99092358450712914, 0.98894894821977797, 0.98375463561338761, 0.98834062369201958, 0.98364431207864977, 0.97777769045786811, 0.9972775598664646, 0.92039565010463043, 0.99301590042596954, 0.98359478335801942, 0.93596352773794445, 0.97803123683052773, 0.83649884354343351, 0.84551606907344867, 0.8735532725329096, 0.95517306093156884, 0.9907035492907057, 0.98661939735885196, 0.80060520918957234, 0.99036234878015061, 0.84346139190131075, 0.94009834867697184, 0.98502180946092033, 0.90345220849498742, 0.83413780295075424, 0.82942395796458124, 0.75929851887448818, 0.90968365292274012, 0.98748432555741439, 0.94175959118684771, 0.85356288535561231, 0.96179638367814646, 0.97581938069440022, 0.98817547403178785, 0.98169571868302041, 0.9852349134184224, 0.98994344246047516, 0.98957985657559244, 0.98363289044164848, 0.99239754866383623, 0.98497972977181747, 0.86548715193242365, 0.89436025204681313, 0.98283547590218312, 0.91133645451931922, 0.88592616827949788, 0.8704845274388221, 0.93427139667086045, 0.98012681406841984, 0.96223889820979436, 0.94693548735284838, 0.97732721897623298, 0.92744786722856065, 0.95871728594335215, 0.97561622842772766, 0.95090820251467267, 0.97568249541217722, 0.85664292070697612, 0.91815862949644389]
computed_metrics: [0.71491312515613648, 0.9089717388339964, 0.87819499194524242, 0.82653679273675762, 0.86139624580482343, 0.86051655546985995, 0.92621410157171635, 0.77458865516022291, 0.89872930548976648, 0.83497495322590387, 0.51612313875947746, 0.70838251305170064, 0.83426215579500251, 0.89374376910127973, 0.70633297721591859, 0.90363476550777921, 0.89577839980665797, 0.83111876066089874, 0.79320033485549368, 0.93730349344978148, 0.69302032394651181, 0.82041103934233695, 0.81442732656446504, 0.84426401259334027, 0.86042530664356764, 0.822703770795425, 0.88334160541778983, 0.67538812785388136, 0.64227629197667402, 0.86672038379915617, 0.89468930493416932, 0.86633110469533503, 0.73735342567886386, 0.79595061677790202, 0.69100457904884316, 0.90577624894297859, 0.91098182159203134, 0.8304321959027402, 0.71632504426631471, 0.64471579145492186, 0.71679955478397228, 0.83312667843541899, 0.84554792695127357, 0.80378470606483643, 0.65480605544747084, 0.77968704634709129, 0.76379432548622328, 0.77064945877930235, 0.84234796983896287, 0.83780970389210552, 0.77609859564456463, 0.89417953676104278, 0.81035249915776519, 0.65964059882978798, 0.56926175220257536, 0.69151275359596398, 0.84701962427717892, 0.56514439684952333, 0.80718815279021183, 0.89574993811820924, 0.88976142796830338, 0.86966038037725946, 0.48326983386465544, 0.89221289324359776, 0.81870157711741864, 0.88483684095693338, 0.89714340906260626, 0.86157191960885293, 0.67208987725005942, 0.7332005242002867, 0.75627043628292423, 0.88503088492010207, 0.51783855922387834, 0.82058718862863267, 0.8558261653475645, 0.93528531891186484, 0.85917774845251882, 0.6194967692476181, 0.64560781518251531, 0.73339259445071137, 0.54358759500107046, 0.79385542279898025, 0.77791037343201297, 0.79902804522800741, 0.87878218828849253, 0.89680365568582054, 0.88337699017349247, 0.73335659021376687, 0.75448221684426597, 0.78683924040993347, 0.86414210776020117, 0.72674883964843162, 0.82808145184984294, 0.81751157382744122, 0.8102891612175972, 0.68958496631060617, 0.79672801293381457, 0.92501043104278335, 0.59706761747473169, 0.72148888359772601, 0.81724067952662494, 0.83526116910867809, 0.75140259657743136, 0.78854239663629988, 0.85879945464835117, 0.77845188550235611, 0.89701220506888524, 0.5201546855600232, 0.60021609042553192, 0.90738609320237118, 0.80360544865688699, 0.84727508650519034, 0.79949996527536626, 0.81002301002301014, 0.84943885448916412, 0.81520215352034953, 0.82082829046898653, 0.86688844479105742, 0.81402961808261887, 0.71418649936530931, 0.78812158501308605, 0.85482848265804323, 0.84478003835358961, 0.7793987486647338, 0.57084800359577503, 0.45602652481931161, 0.74968068707333191, 0.72143261455525609]
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ Constandse, Kayla Michelle
bash: Constandse,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ Akre, Alex Vivian
bash: Akre,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ Cathers, Phillip Hamilton
bash: Cathers,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ Carrera, Bella Abigail
bash: Carrera,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ Davis, Christian KeShawn
bash: Davis,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ McNiel, Chase Thomas
bash: McNiel,: command not found
]0;zqwu@sherlock-ln04:~/deepchem/examples\(deepchem) [[1m[32mzqwu[m@gpu-9-4 ~/deepchem/examples]$ O'Keeffe, Fiona Rhiannon
> Vesey, Annie Sinclair
> Schaepe, Julia Marie
> Cremers, Collin John
> Rorick, Medora Evelyn
> Cho, Young Joon
> Elifas, Alex
> Chiragzada, Selin
> Anders, Rachel Sounthone
> O'Keeffe, Fiona Rhiannon
Vesey, Annie Sinclair
Schaepe, Julia Marie
Cremers, Collin John
Rorick, Medora Evelyn
Cho, Young Joon
Elifas, Alex
Chiragzada, Selin
Anders, Rachel SounthoneMMMMMMMM[C[C[K
[K
[K
[K
[K
[K
[K
[K
[KMMMMMMMM[C[C
> 
> aa
> 